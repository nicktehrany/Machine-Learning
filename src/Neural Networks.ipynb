{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/nicktehrany/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/nicktehrany/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd # for loading data\n",
    "import re # for removing special characters\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords # For removing stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"dataset.csv\")\n",
    "df = df.drop(['Singer', 'Date', 'Tags'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_index(word, word_list):\n",
    "    for index, value in enumerate(word_list):\n",
    "        if word == value:\n",
    "            return index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a word list of all possible words from all Song Lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "lyrics = df['Lyrics']\n",
    "sw = stopwords.words(\"english\")\n",
    "word_list = []\n",
    "for i, value in df.iterrows():\n",
    "    text = re.sub('[^a-zA-Z]', ' ', str(lyrics[i])) # removes special characters\n",
    "    text = text.lower() # lowercases everything\n",
    "    text = text.split() # splits words\n",
    "    text = [wordnet_lemmatizer.lemmatize(word, pos=\"v\") for word in text if not word in set(sw)]\n",
    "    formatted_text = \"\"\n",
    "    for word in text:\n",
    "        if word not in word_list:\n",
    "            word_list.append(word)\n",
    "        formatted_text+=word+\" \"\n",
    "    corpus.append(formatted_text)\n",
    "    df['Lyrics'][i] = formatted_text\n",
    "lyrics = corpus\n",
    "\n",
    "#Cleaning variables to save some memory\n",
    "del corpus, sw, text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function that creates a one-hot vector for the given text from the word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nicktehrany/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/nicktehrany/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/nicktehrany/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/nicktehrany/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/nicktehrany/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/nicktehrany/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/nicktehrany/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/nicktehrany/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/nicktehrany/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/nicktehrany/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/nicktehrany/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/nicktehrany/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "def one_hot(text, word_list):\n",
    "    word_vector = np.zeros(shape=(1,len(word_list)))\n",
    "    text = text.split() # splits words\n",
    "    for w in text:\n",
    "        word_vector[0, get_index(w, word_list)] = 1\n",
    "    return word_vector[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Creating one-hot vector for genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "genres = []\n",
    "\n",
    "for i, value in df.iterrows():\n",
    "    genre = df['Genre'][i]\n",
    "    index = 0\n",
    "    if genre == '[\\'Pop\\']': index = 0\n",
    "    elif genre == '[\\'Rock\\']': index = 1\n",
    "    elif genre == '[\\'Hip-Hop/Rap\\']': index = 2\n",
    "    elif genre == '[\\'Country\\']': index = 3\n",
    "    elif genre == '[\\'R&B/Soul\\']': index = 4\n",
    "    elif genre == '[\\'Metal\\']': index = 5\n",
    "    elif genre == '[\\'Alternative/Indie\\']': index = 6\n",
    "    elif genre == '[\\'Folk\\']': index = 7\n",
    "    genres.append(index)\n",
    "\n",
    "genres = to_categorical(genres, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replace all the lyrics with their respctive vector of word occurrences to be used as the input layer for the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14400, 40677)\n"
     ]
    }
   ],
   "source": [
    "all_lyrics = np.zeros(shape=(len(df),len(word_list)))\n",
    "for index, value in df.iterrows():\n",
    "    all_lyrics[index] = one_hot(str(df['Lyrics'][index]), word_list)\n",
    "print(all_lyrics.shape)\n",
    "\n",
    "#Cleaning variables to save some memory\n",
    "del df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Don't rerun the cell below, as this will result in new test/train sets. Can rerun all other cells except this one. Probably also don't want to rerun all cells above since they always do the exact same thing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "\n",
    "lyrics_train, lyrics_test, genre_train, genre_test = train_test_split(all_lyrics, genres, train_size = 0.85, test_size = 0.15, shuffle=True,random_state=random.randint(0,9999999)) \n",
    "\n",
    "#Cleaning Lists, since they take up almost 5GB\n",
    "del all_lyrics, genres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network stuff \n",
    "## **Modify and run only the cells below to change the neural network**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Large dropout rate: 0.8 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_6 (Dense)              (None, 128)               5206784   \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 20)                2580      \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 8)                 168       \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 8)                 0         \n",
      "=================================================================\n",
      "Total params: 5,209,532\n",
      "Trainable params: 5,209,532\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(units=128, input_shape=(len(word_list),)))\n",
    "model.add(Activation('relu'))            # activation layer\n",
    "model.add(Dropout(0.8))\n",
    "model.add(Dense(20))\n",
    "model.add(Dense(8))\n",
    "model.add(Activation('softmax'))         # output class probabilities\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "\n",
    "optimizer = Adam(lr=0.001) # lr is the learning rate\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11016 samples, validate on 1224 samples\n",
      "Epoch 1/10\n",
      "11016/11016 [==============================] - 10s 877us/sample - loss: 1.7106 - acc: 0.3552 - val_loss: 1.4314 - val_acc: 0.4837\n",
      "Epoch 2/10\n",
      "11016/11016 [==============================] - 9s 860us/sample - loss: 1.4198 - acc: 0.4749 - val_loss: 1.3353 - val_acc: 0.5237\n",
      "Epoch 3/10\n",
      "11016/11016 [==============================] - 9s 858us/sample - loss: 1.2503 - acc: 0.5428 - val_loss: 1.3059 - val_acc: 0.5270\n",
      "Epoch 4/10\n",
      "11016/11016 [==============================] - 9s 861us/sample - loss: 1.1420 - acc: 0.5891 - val_loss: 1.2829 - val_acc: 0.5384\n",
      "Epoch 5/10\n",
      "11016/11016 [==============================] - 9s 855us/sample - loss: 1.0534 - acc: 0.6142 - val_loss: 1.2973 - val_acc: 0.5368\n",
      "Epoch 6/10\n",
      "11016/11016 [==============================] - 10s 875us/sample - loss: 0.9876 - acc: 0.6376 - val_loss: 1.2973 - val_acc: 0.5278\n",
      "Epoch 7/10\n",
      "11016/11016 [==============================] - 10s 868us/sample - loss: 0.9122 - acc: 0.6724 - val_loss: 1.3313 - val_acc: 0.5343\n",
      "Epoch 8/10\n",
      "11016/11016 [==============================] - 10s 888us/sample - loss: 0.8593 - acc: 0.6846 - val_loss: 1.3208 - val_acc: 0.5449\n",
      "Epoch 9/10\n",
      "11016/11016 [==============================] - 10s 932us/sample - loss: 0.8238 - acc: 0.6966 - val_loss: 1.3372 - val_acc: 0.5384\n",
      "Epoch 10/10\n",
      "11016/11016 [==============================] - 10s 923us/sample - loss: 0.7881 - acc: 0.7165 - val_loss: 1.3821 - val_acc: 0.5368\n"
     ]
    }
   ],
   "source": [
    "model.fit(lyrics_train, genre_train, epochs=10, validation_split=1/10, batch_size=16);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the Neural Network's performace + Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2160/2160 [==============================] - 1s 249us/sample - loss: 1.3942 - acc: 0.5472\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(lyrics_test, genre_test, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_reverse():\n",
    "    genres = []\n",
    "    genre_list = list(genre_test)\n",
    "\n",
    "    for i in range(len(genre_list)):\n",
    "        x = genre_list[i]\n",
    "        for index in range(0, 8):\n",
    "            if x[index] == 1.0:\n",
    "                genres.append(index)\n",
    "\n",
    "    return genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 87  53  13  23  30  12  22  24]\n",
      " [ 30 121   0  19   6  25  26  19]\n",
      " [  3   2 207   0  23   1  10   3]\n",
      " [  9  36   0 189   7   2  14  32]\n",
      " [ 16  23  25  16 150   2  43  11]\n",
      " [  6  60   2   4   1 169  16  22]\n",
      " [  7  34  21   6  30  14 122  30]\n",
      " [  8  47   0  52   2  11  25 137]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_pred=model.predict_classes(lyrics_test)\n",
    "cm=confusion_matrix(one_hot_reverse(),y_pred)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   Pop  Rock  Hip-Hop/Rap  Country  R&B/Soul  Metal  \\\n",
      "Pop                 87    53           13       23        30     12   \n",
      "Rock                30   121            0       19         6     25   \n",
      "Hip-Hop/Rap          3     2          207        0        23      1   \n",
      "Country              9    36            0      189         7      2   \n",
      "R&B/Soul            16    23           25       16       150      2   \n",
      "Metal                6    60            2        4         1    169   \n",
      "Alternative/Indie    7    34           21        6        30     14   \n",
      "Folk                 8    47            0       52         2     11   \n",
      "\n",
      "                   Alternative/Indie  Folk  \n",
      "Pop                               22    24  \n",
      "Rock                              26    19  \n",
      "Hip-Hop/Rap                       10     3  \n",
      "Country                           14    32  \n",
      "R&B/Soul                          43    11  \n",
      "Metal                             16    22  \n",
      "Alternative/Indie                122    30  \n",
      "Folk                              25   137  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(8.0, 0.0)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "genre_list = ['Pop','Rock', 'Hip-Hop/Rap', 'Country', 'R&B/Soul', 'Metal', 'Alternative/Indie', 'Folk']\n",
    "\n",
    "df_cm = pd.DataFrame(cm, index = genre_list, columns = genre_list)\n",
    "print(df_cm)\n",
    "plt.figure(figsize = (10,7))\n",
    "sn.heatmap(df_cm, annot=True, linewidth=0.5, fmt='g', cmap=\"BuPu\")\n",
    "b, t = plt.ylim() # discover the values for bottom and top\n",
    "b += 0.5 # Add 0.5 to the bottom\n",
    "t -= 0.5 # Subtract 0.5 from the top\n",
    "plt.ylim(b, t) # update the ylim(bottom, top) values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Don't rerun the cell below, as this will result in adding dimesnions to the sets, which will not work! Can rerun all other cells except this one.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_lyrics_train=lyrics_train[:, :, None]\n",
    "cnn_lyrics_test=lyrics_test[:, :, None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_2 (Conv1D)            (None, 40677, 16)         32        \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 40677, 16)         272       \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 40677, 16)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 650832)            0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 32)                20826656  \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 8)                 264       \n",
      "=================================================================\n",
      "Total params: 20,827,224\n",
      "Trainable params: 20,827,224\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Conv1D, MaxPool2D, Dropout, Flatten\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv1D(16, kernel_size=(1), activation='relu', input_shape=(len(word_list),1)))\n",
    "model.add(Conv1D(16, (1), activation='relu'))\n",
    "model.add(Dropout(0.25)) # Dropout 25% of the nodes of the previous layer during training\n",
    "model.add(Flatten())     # Flatten, and add a fully connected layer\n",
    "model.add(Dense(32, activation='relu')) \n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(8, activation='softmax')) # Last layer: 10 class nodes, with dropout\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10200 samples, validate on 2040 samples\n",
      "Epoch 1/5\n",
      "10200/10200 [==============================] - 106s 10ms/sample - loss: 1.7627 - acc: 0.3341 - val_loss: 1.4821 - val_acc: 0.4819\n",
      "Epoch 2/5\n",
      "10200/10200 [==============================] - 104s 10ms/sample - loss: 1.3858 - acc: 0.4859 - val_loss: 1.3478 - val_acc: 0.5181\n",
      "Epoch 3/5\n",
      "10200/10200 [==============================] - 104s 10ms/sample - loss: 1.1761 - acc: 0.5629 - val_loss: 1.3478 - val_acc: 0.5167\n",
      "Epoch 4/5\n",
      "10200/10200 [==============================] - 103s 10ms/sample - loss: 1.0109 - acc: 0.6184 - val_loss: 1.3699 - val_acc: 0.5176\n",
      "Epoch 5/5\n",
      "10200/10200 [==============================] - 103s 10ms/sample - loss: 0.8653 - acc: 0.6725 - val_loss: 1.4451 - val_acc: 0.5235\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f91f012c668>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "optimizer = Adam()\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model, iterating on the data in batches of 32 samples\n",
    "model.fit(cnn_lyrics_train, genre_train, epochs=5, batch_size=32, validation_split=1/6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2160/2160 [==============================] - 3s 2ms/sample - loss: 1.4622 - acc: 0.5273\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(cnn_lyrics_test, genre_test, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
