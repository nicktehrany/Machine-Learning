{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package genesis to\n",
      "[nltk_data]     /Users/thomaswierda/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/genesis.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/thomaswierda/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/thomaswierda/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/thomaswierda/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import genesis\n",
    "nltk.download('genesis')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "genesis_ic = wn.ic(genesis, False, 0.0)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.metrics import roc_auc_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNN_NLC_Classifer():\n",
    "    def __init__(self, k=1, distance_type = 'path'):\n",
    "        self.k = k\n",
    "        self.distance_type = distance_type\n",
    "\n",
    "    # This function is used for training\n",
    "    def fit(self, x_train, y_train):\n",
    "        self.x_train = x_train\n",
    "        self.y_train = y_train\n",
    "\n",
    "    # This function runs the K(1) nearest neighbour algorithm and\n",
    "    # returns the label with closest match. \n",
    "    def predict(self, x_test):\n",
    "        self.x_test = x_test\n",
    "        y_predict = []\n",
    "\n",
    "        for i in range(len(x_test)):\n",
    "            max_sim = 0\n",
    "            max_index = 0\n",
    "            for j in range(self.x_train.shape[0]):\n",
    "                temp = document_similarity(self,x_test[i], self.x_train[j])\n",
    "                if temp > max_sim:\n",
    "                    max_sim = temp\n",
    "                    max_index = j\n",
    "            y_predict.append(self.y_train[max_index])\n",
    "        return y_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_tag(self, tag):\n",
    "        \"\"\"Convert the tag given by nltk.pos_tag to the tag used by wordnet.synsets\"\"\"\n",
    "        tag_dict = {'N': 'n', 'J': 'a', 'R': 'r', 'V': 'v'}\n",
    "        try:\n",
    "            return tag_dict[tag[0]]\n",
    "        except KeyError:\n",
    "            return None\n",
    "\n",
    "\n",
    "def doc_to_synsets(self, doc):\n",
    "    \"\"\"\n",
    "        Returns a list of synsets in document.\n",
    "        Tokenizes and tags the words in the document doc.\n",
    "        Then finds the first synset for each word/tag combination.\n",
    "        If a synset is not found for that combination it is skipped.\n",
    "\n",
    "        Args:\n",
    "            doc: string to be converted\n",
    "\n",
    "        Returns:\n",
    "            list of synsets\n",
    "     \"\"\"\n",
    "    tokens = word_tokenize(doc+' ')\n",
    "        \n",
    "    l = []\n",
    "    tags = nltk.pos_tag([tokens[0] + ' ']) if len(tokens) == 1 else nltk.pos_tag(tokens)\n",
    "        \n",
    "    for token, tag in zip(tokens, tags):\n",
    "        syntag = self.convert_tag(tag[1])\n",
    "        syns = wn.synsets(token, syntag)\n",
    "        if (len(syns) > 0):\n",
    "             l.append(syns[0])\n",
    "    return l "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity_score(self, s1, s2, distance_type = 'path'):\n",
    "        \"\"\"\n",
    "          Calculate the normalized similarity score of s1 onto s2\n",
    "          For each synset in s1, finds the synset in s2 with the largest similarity value.\n",
    "          Sum of all of the largest similarity values and normalize this value by dividing it by the\n",
    "          number of largest similarity values found.\n",
    "\n",
    "          Args:\n",
    "              s1, s2: list of synsets from doc_to_synsets\n",
    "\n",
    "          Returns:\n",
    "            normalized similarity score of s1 onto s2\n",
    "        \"\"\"\n",
    "        s1_largest_scores = []\n",
    "\n",
    "        for i, s1_synset in enumerate(s1, 0):\n",
    "            max_score = 0\n",
    "            for s2_synset in s2:\n",
    "                if distance_type == 'path':\n",
    "                      score = s1_synset.path_similarity(s2_synset, simulate_root = False)\n",
    "                else:\n",
    "                      score = s1_synset.wup_similarity(s2_synset)                  \n",
    "                if score != None:\n",
    "                    if score > max_score:\n",
    "                          max_score = score\n",
    "              \n",
    "            if max_score != 0:\n",
    "                s1_largest_scores.append(max_score)\n",
    "          \n",
    "        mean_score = np.mean(s1_largest_scores)\n",
    "                 \n",
    "        return mean_score  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_similarity(self,doc1, doc2):\n",
    "    \"\"\"Finds the symmetrical similarity between doc1 and doc2\"\"\"\n",
    "    synsets1 = self.doc_to_synsets(doc1)\n",
    "    synsets2 = self.doc_to_synsets(doc2)\n",
    "          \n",
    "    return (self.similarity_score(synsets1, synsets2) + self.similarity_score(synsets2, synsets1)) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Singer                            Song                  Genre  \\\n",
      "0  $UICIDEBOY$                  40 Oz. & Sober  ['Alternative/Indie']   \n",
      "1  $UICIDEBOY$           Drugs/Hoes/Money/Etc.        ['Hip-Hop/Rap']   \n",
      "2  $UICIDEBOY$  Iâ€™ll Pay for It (If I Want It)        ['Hip-Hop/Rap']   \n",
      "3  $UICIDEBOY$        Leave Your Things Behind        ['Hip-Hop/Rap']   \n",
      "4  $UICIDEBOY$              Rag Round My Skull        ['Hip-Hop/Rap']   \n",
      "\n",
      "                                              Lyrics                    Tags  \\\n",
      "0  \\n\\n\\nKiller, killer, killer\\nWhat the fuck yo...  ['Alternative', 'Rap']   \n",
      "1  \\n\\n\\nCounting sheep until I'm sound asleep\\nI...                 ['Rap']   \n",
      "2  \\n\\n\\nFucking Ruby got a blunt with a flame at...                 ['Rap']   \n",
      "3  \\n\\n\\nAre you in or out?\\nAre you in or out?\\n...                 ['Rap']   \n",
      "4  \\n\\n\\nDrugs in my veins with two hoes on my la...                 ['Rap']   \n",
      "\n",
      "   output  \n",
      "0       6  \n",
      "1       2  \n",
      "2       2  \n",
      "3       2  \n",
      "4       2  \n",
      "\n",
      "Size of input file is  (14399, 6)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "FILENAME = '/Users/thomaswierda/Documents/Study/Machine learning/dataset.csv'\n",
    "df = pd.read_csv(FILENAME)\n",
    "df = df.drop(['Singer','Tags'], axis=1)\n",
    "genres = []\n",
    "\n",
    "for i, value in df.iterrows():\n",
    "    genre = df['Genre'][i]\n",
    "    index = 0\n",
    "    if genre == '[\\'Pop\\']': index = 0\n",
    "    elif genre == '[\\'Rock\\']': index = 1\n",
    "    elif genre == '[\\'Hip-Hop/Rap\\']': index = 2\n",
    "    elif genre == '[\\'Country\\']': index = 3\n",
    "    elif genre == '[\\'R&B/Soul\\']': index = 4\n",
    "    elif genre == '[\\'Metal\\']': index = 5\n",
    "    elif genre == '[\\'Alternative/Indie\\']': index = 6\n",
    "    elif genre == '[\\'Folk\\']': index = 7\n",
    "    genres.append(index)\n",
    "\n",
    "dataset['output'] = genres\n",
    "Num_Words = dataset.shape[0]\n",
    "\n",
    "print(dataset.head())\n",
    "print(\"\\nSize of input file is \", dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-61-c5065b804756>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0my_pred_final\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlyrics_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0moutput_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfinal_test_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'code'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0my_pred_final\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-56-9cf451291a7d>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x_test)\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mmax_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m                 \u001b[0mtemp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdocument_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtemp\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mmax_sim\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m                     \u001b[0mmax_sim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtemp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    866\u001b[0m         \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_if_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    867\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 868\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    869\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_value\u001b[0;34m(self, series, key)\u001b[0m\n\u001b[1;32m   4373\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4374\u001b[0m             return self._engine.get_value(s, k,\n\u001b[0;32m-> 4375\u001b[0;31m                                           tz=getattr(series.dtype, 'tz', None))\n\u001b[0m\u001b[1;32m   4376\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4377\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mholds_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_boolean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_value\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_value\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "import re \n",
    "# 4. Train the Classifier\n",
    "classifier = KNN_NLC_Classifer(k=1, distance_type='path')\n",
    "lyrics = df['Lyrics']\n",
    "genre = df['Genre']\n",
    "lyrics_train, lyrics_test, genre_train, genre_test = train_test_split(lyrics, genre, train_size = 0.85, test_size = 0.15, shuffle=True)\n",
    "\n",
    "\n",
    "classifier.fit(lyrics_train, genre_train)\n",
    "\n",
    "\n",
    "y_pred_final = classifier.predict(lyrics_test)\n",
    "\n",
    "output_df = pd.DataFrame(data = {'text': final_test_list, 'code': y_pred_final})\n",
    "print(output_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
